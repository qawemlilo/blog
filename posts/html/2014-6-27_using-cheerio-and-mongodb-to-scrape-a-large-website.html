<!DOCTYPE html>
<html>
<head>

<meta charset="utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Scrapping websites using node.js">
<meta name="author" content="Qawelesizwe Mlilo">
<meta name="generator" content="Node Blogger" />

<title>Using Cheerio and MongoDB to scrape a large website</title>

<link rel="alternate" type="application/rss+xml" title="RSS Feed for blg.ragingflame.co.za" href="/rss" />
<link rel="shortcut icon" type="image/x-icon" href="/favicon.ico">
<link href="/css/style_v1479116714375.css" rel="stylesheet">
<link href="http://fonts.googleapis.com/css?family=Open+Sans+Condensed:700&amp;subset=latin,cyrillic-ext" rel="stylesheet" />

<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
<!--[if lt IE 9]>
  <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
  <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="navmenu navmenu-default navmenu-fixed-left offcanvas-sm">

      <button type="button" style="margin-right: 10px; margin-top: 5px; color:#fff" class="close hidden-md hidden-lg pull-right" aria-label="Close" data-toggle="offcanvas" data-target=".navmenu">
        <span aria-hidden="true">&times;</span>
      </button>

      <div class="text-center avatar">
        <a href="/">
          <img src="/img/q-mobile.png" class="img-circle desktop" title="Qawelesizwe Mlilo" alt="Qawelesizwe Mlilo" />
        </a>
      </div>

      <p class="description">
        Hi, my name is Qawelesizwe Mlilo (or Que). I'm a Lead Developer at a UK based Startup working remotely. This blog is about my programming (adventures || misadventures).<br><br><a href="https://github.com/qawemlilo/blog" target="_blank">Please send me a pull request for any corrections.</a>
      </p>

      <ul class="nav navmenu-nav">
        <li>
          <a href="/">Home</a>
        </li>
        <li >
          <a href="/about">About</a>
        </li>
        <li>
          <a href="https://github.com/qawemlilo" target="_blank">Projects</a>
        </li>
        <li>
          <a href="mailto:qawemlilo@gmail.com">Contact Me</a>
        </li>
      </ul>

      <ul class="list-inline social-media">
        <li>
          <a target="_blank" href="https://github.com/qawemlilo" title="Github" class="github">&nbsp;</a>
        </li>
        <li>
          <a href="http://twitter.com/ragingflameblog" title="Follow me on Twitter" class="twitter">&nbsp;</a>
        </li>
        <li>
          <a href="https://plus.google.com/111595084798587457827/posts?hl=en&amp;partnerid=gplp0" title="Google+" class="google">&nbsp;</a>
        </li>
        <li>
          <a href="/rss" title="RSS" class="rss">&nbsp;</a>
        </li>
      </ul>
    </div>

    <div class="navbar navbar-inverse navbar-fixed-top hidden-md hidden-lg">
      <button type="button" class="navbar-toggle" data-toggle="offcanvas" data-target=".navmenu">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Raging Flame Blog</a>
    </div>

    <div class="container content">
      <div class="row">
        <div class="post blogpost">
          <h1>Using Cheerio and MongoDB to scrape a large website</h1>

          <div class="row date">
            <div class="dt col-xs-12 col-sm-8 col-md-8 col-lg-8">
              <span style="margin-right:10px">
                27
                Jun
                <a href="/2014">2014</a>
              </span>

              
                  &nbsp;
                  <span class="label label-default">
                    <a href="/tags/nodejs">nodejs</a>
                  </span>
              
                  &nbsp;
                  <span class="label label-default">
                    <a href="/tags/node">node</a>
                  </span>
              
                  &nbsp;
                  <span class="label label-default">
                    <a href="/tags/cheerio">cheerio</a>
                  </span>
              
                  &nbsp;
                  <span class="label label-default">
                    <a href="/tags/mongodb">mongodb</a>
                  </span>
              
                  &nbsp;
                  <span class="label label-default">
                    <a href="/tags/scrapping">scrapping</a>
                  </span>
              
            </div>
            <div class="sharep col-xs-12 col-sm-4 col-md-4 col-lg-4" style="text-align:right">
              <ul class="list-inline list-unstyled">
                <li>
                  <a target="_blank" href="http://www.facebook.com/share.php?u=https://blog.ragingflame.co.za/2014/6/27/using-cheerio-and-mongodb-to-scrape-a-large-website" class="share-fb" title="Share on Facebook">&nbsp;</a>
                </li>

                <li>
              <a target="_blank" href="http://twitter.com/home?status=https://blog.ragingflame.co.za/2014/6/27/using-cheerio-and-mongodb-to-scrape-a-large-website" class="share-twitter" title="Share on Twitter">
               &nbsp;
              </a>
                </li>

                <li>
              <a target="_blank" href="http://reddit.com/submit?url=https://blog.ragingflame.co.za/2014/6/27/using-cheerio-and-mongodb-to-scrape-a-large-website" class="share-reddit" title="Share on Reddit">
               &nbsp;
              </a>
                </li>

                <li>
              <a target="_blank" href="http://www.linkedin.com/shareArticle?mini=true&url=https://blog.ragingflame.co.za/2014/6/27/using-cheerio-and-mongodb-to-scrape-a-large-website" class="share-linkedin" title="Share on LinkedIn">
               &nbsp;
              </a>
                </li>
              </ul>

            </div>
          </div>

          <p><em>Edit:</em> Fixed spelling errors.</p>
<p>A friend of mine is building a web application that provides services to local businesses and he needed to collect contact details of as many companies as possible.
After doing some research, he discovered that the best resource that provided information about local business was our Yellow Pages website. In his infinite wisdom, he decided to recruit me to write a bot that collected all the data he required from the Yellow Pages website.</p>
<p><em>Disclaimer:</em> Scraping has a moral grey area, as much as the information on a website is publicly available, I don't think that mass cloning the data is morally upright.</p>
<h3>The Plan</h3>
<p>Building a scraping bot requires 2 main things, a collection of urls that you want to scrape and a module that parses HTML. In this particular project we were very lucky because the website we were scraping had well structured urls that seemed to use database id fields to load information for a particular business.</p>
<p>The urls looked something like this <code>http://localyellowpages.com/listing/27</code>. All I had to do was run a loop that created all urls starting from 1 and stopping at largest id that we found.</p>
<p>The function that took care of creating our url collection looks like this:</p>
<pre><code>function generateUrls(limit) {
  var url = 'http://localyellowpages.com/listing/';
  var urls = [];
  var i;

  for (i=1; i &lt; limit; i++) {
    urls.push(url + i);
  }

  return urls;
}
</code></pre>
<p>Once the collection was created I needed to load each page individually using the http module, parse the HTML to access the contact details of a business and then store the details in MongoDB.</p>
<h3>Choosing Cheerio</h3>
<p><a href="https://github.com/cheeriojs/cheerio">Cheerio</a> is an awesome server side DOM manipulation module, I have used it in previous projects like <a href="http://blog.ragingflame.co.za/2013/2/8/crushit">CrushIt</a> and totally loved it. With cheerio, you can access DOM elements the same way you do with jQuery on the browser. Here is an example:</p>
<pre><code>var cheerio = require('cheerio');
var html = '&lt;div&gt;&lt;ul&gt;&lt;li&gt;1&lt;/li&gt;&lt;li id=&quot;mynum&quot;&gt;2&lt;/li&gt;&lt;li&gt;3&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;';

var $ = cheerio.load(html);

// get my number
var mynum = $('#mynum').text();

console.log(mynum) // logs 2
</code></pre>
<p>Easy as pie!</p>
<h3>The Bot</h3>
<p>Armed with cheerio and a hacker attitude I went straight to work. The largest id we found was 25000, meaning that the website had about that number of entries minus any that were deleted.</p>
<h3>Dependencies</h3>
<ul>
<li>Cheerio - for parsing HTML</li>
<li>Mongoose - for storing our data in MongoDB</li>
</ul>
<p>The bot comprises of 4 files:</p>
<ul>
<li><code>bot.js</code> - the main execution file</li>
<li><code>model.js</code> - MongoDB model for storing data</li>
<li><code>scraper.js</code> - scraping constructor</li>
<li><code>package.json</code> - metadata</li>
</ul>
<h3>Model.js</h3>
<p>Creating a model first is good because it defines clearly the data that you want to collect, in this case we wanted detailed contact information.</p>
<pre><code>var mongoose = require('mongoose');

mongoose.connect('mongodb://localhost:27017/scraper');
mongoose.connection.on('error', function() {
  console.error('MongoDB Connection Error. Make sure MongoDB is running.');
});

var ListingsSchema = new mongoose.Schema({
  title: String,
  email: {type: String, lowercase: true},
  cell: Array,
  telephone: Array,
  fax: Array,
  website: {type: String, default: '', lowercase: true},
  postalAddress: Array,
  address: Array,
  url: String
});

module.exports = mongoose.model('Listings', ListingsSchema);
</code></pre>
<p>The model basically defines all fields we want to collect - <code>cell</code>, <code>telephone</code> and <code>fax</code> are arrays because there were cases where a business would have more than one number. The <code>address</code> and <code>postalAddress</code> were also split into arrays for convenience when querying.</p>
<h3>Scraper.js</h3>
<p>My first attempt at loading pages and parsing them was a disaster, the bot tried to load all 25000 pages at the same time. After some deliberation I decided to  create the Scraper as an EventEmitter and Constructor that fired a <code>complete</code> event when it was done parsing a page. This would make it possible to process my urls in batches.</p>
<pre><code>var http = require('http');
var cheerio = require('cheerio');
var util = require('util');
var EventEmitter = require('events').EventEmitter;
var STATUS_CODES = http.STATUS_CODES;


/*
 * Scraper Constructor
**/
function Scraper (url) {
    this.url = url;

    this.init();
}


/*
 * Make it an EventEmitter
**/
util.inherits(Scraper, EventEmitter);
</code></pre>
<p>I started by defining my Scraper Constructor and turning it into an EventEmitter. When instantiated, it calls the <code>init</code> method.</p>
<p>Up next let's look at the <code>init</code> method.</p>
<pre><code>/*
 * Initialize scraping
**/
Scraper.prototype.init = function () {
    var model;
    var self = this;

    self.on('loaded', function (html) {
        model = self.parsePage(html);
        self.emit('complete', model);
    });

    self.loadWebPage();
};
</code></pre>
<p>The <code>init</code> method attaches an event listener to the Scraper's <code>loaded</code> event, when that event fires, it comes with some HTML which is parsed before being passed to a <code>complete</code> event listener. Lastly the <code>init</code> method calls the <code>loadWebPage</code> method.</p>
<pre><code>Scraper.prototype.loadWebPage = function () {
  var self = this;

  console.log('\n\nLoading ' + website);

  http.get(self.url, function (res) {
    var body = '';
   
    if(res.statusCode !== 200) {
      return self.emit('error', STATUS_CODES[res.statusCode]);
    }

    res.on('data', function (chunk) {
      body += chunk;
    });

    res.on('end', function () {
      self.emit('loaded', body);
    });
  })
  .on('error', function (err) {
    self.emit('error', err);
  });      
};


/*
 * Parse html and return an object
**/
Scraper.prototype.parsePage = function (html) {
  var $ = cheerio.load(html);

  var address = $('#address').text();
  var tel = $('#tel').text();
  var cell = $('#cell').text();
  var fax = $('#fax').text();
  var email = $('#email').text();
  var website = $('#website').attr('href');
  var postal =  $('#postal').text();

  var model = {
    title: address.trim().split('\n'),
    email: email.trim(),
    cell: cell.trim().split('\n'),
    telephone: tel.trim().split('\n'),
    fax: fax.trim().split('\n'),
    website: website || '',
    postalAddress: postal.trim().split('\n'),
    address: address.trim().split('\n'),
    url: this.url
  };

  return model;
};


module.exports = Scraper;
</code></pre>
<p><code>loadWebPage</code> is pretty straight forward, it loads a web page using the native <code>http</code> module and then fires the <code>loaded</code> event once complete.</p>
<p><code>parsePage</code> is used to traverse the HTML DOM and gather the required data - it really depends on the page layout, you should first check the source code of the pages you are parsing before creating your own parser method.</p>
<h3>bot.js</h3>
<p>The final piece of the puzzle is completed in <code>bot.js</code>, we first require our model and scraper. Earlier on I created the <code>generateUrls</code> function which is also included in <code>bot.js</code>.</p>
<pre><code>var Model = require('./model');
var Scraper = require('./scraper');
var Pages = [];


function generateUrls(limit) {
  var url = 'http://localyellowpages.com/listing/';
  var urls = [];
  var i;

  for (i=1; i &lt; limit; i++) {
    urls.push(url + i);
  }

  return urls;
}


// store all urls in a global variable  
Pages = generateUrls(25000);


function wizard() {
  // if the Pages array is empty, we are Done!!
  if (!Pages.length) {
    return console.log('Done!!!!');
  }

  var url = Pages.pop();
  var scraper = new Scraper(url);
  var model;

  console.log('Requests Left: ' + Pages.length);

  // if the error occurs we still want to create our
  // next request
  scraper.on('error', function (error) {
    console.log(error);
    wizard();
  });

  // if the request completed successfully
  // we want to store the results in our database
  scraper.on('complete', function (listing) {
    model = new Model(listing);
   
    model.save(function(err) {
      if (err) {
        console.log('Database err saving: ' + url);
      }
    });

    wizard();
  });
}
</code></pre>
<p>Details of how things work are contained in the comments - <code>wizard</code> is basically a recursive function that goes on until the <code>Pages</code> array is empty.</p>
<p>Lastly let us create our first batch and fire away!</p>
<pre><code>var numberOfParallelRequests = 20;

for (var i = 0; i &lt; numberOfParallelRequests; i++) {
  wizard();
}
</code></pre>
<p>The variable <code>numberOfParallelRequests</code> works like a tap that controls the speed and number of simultaneous requests being processed at one particular moment.</p>
<pre><code>// run the bot
node bot.js
</code></pre>
<p>And we are done! Sit back, grab a beer and watch Node.js scrape like a BOSS.</p>
<p>Remember, with great power comes a greater responsibility - use Node for good and not for evil. Keep hacking.</p>

        </div>
        <div class="pagination-container">
        
          <a href="/2013/12/10/proposing-a-native-event-emitter-in-javascript" title="Previous Post" class="pagination-arrow newer">Prev</a>
        

        
          <a href="/2014/7/21/using-nodejs-with-mysql" title="Next Post" class="pagination-arrow older">Next</a>
        
      </div>

        <div id="disqus_thread"></div>
        <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      </div>
    </div>

<script src="/js/script_v1479116714375.js"></script>
<script type="text/javascript">
    var disqus_shortname = 'ragingflameblog';
    var disqus_identifier = '/2014/6/27/using-cheerio-and-mongodb-to-scrape-a-large-website';
    var disqus_title = 'Using Cheerio and MongoDB to scrape a large website';
    var disqus_url = 'http://blog.ragingflame.co.za/2014/6/27/using-cheerio-and-mongodb-to-scrape-a-large-website';

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-41105756-1', 'ragingflame.co.za');
  ga('send', 'pageview');

</script>
</body>
</html>
